{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I want to find out which stocks have been in news of the day, or highlighted or trending stocks of day by using news articles\n",
        "#To collect data, I may use free publicly accessed news websiteslike yahoo finance.\n",
        "#After collecting articles related to keywords , I will collect all the text of website.\n",
        "#Then I will use preprocessing techniques to clean data like removing stop words,lematize etc\n",
        "#Then all the quality data will be stored into csv file to use for further analysis.\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Function to scrape news articles from a given URL\n",
        "def scrape_news(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        headlines = soup.find_all('h2')\n",
        "        articles = []\n",
        "        for headline in headlines:\n",
        "            headline_text = headline.get_text().strip()\n",
        "            publication_date = None\n",
        "            date_element = headline.find_previous(class_='publication-date')\n",
        "            if date_element:\n",
        "                publication_date = date_element.get_text().strip()\n",
        "                publication_date = datetime.strptime(publication_date, '%Y-%m-%d %H:%M:%S')\n",
        "            article_content = headline.find_next('p').get_text().strip()\n",
        "            articles.append({'headline': headline_text, 'publication_date': publication_date, 'content': article_content})\n",
        "        return articles\n",
        "    else:\n",
        "        print(\"Failed to retrieve news articles from the URL:\", url)\n",
        "        return []\n",
        "\n",
        "# Function to filter news articles published within the past 24 hours\n",
        "def filter_articles_within_24_hours(articles):\n",
        "    current_time = datetime.now()\n",
        "    filtered_articles = []\n",
        "    for article in articles:\n",
        "        if article['publication_date'] and (current_time - article['publication_date']) < timedelta(days=1):\n",
        "            filtered_articles.append(article)\n",
        "    return filtered_articles\n",
        "\n",
        "# Example usage\n",
        "news_website_url = \"https://finance.yahoo.com\"\n",
        "articles = scrape_news(news_website_url)\n",
        "print(\"Total articles found:\", len(articles))\n",
        "print(\"---------------\")\n",
        "for article in articles:\n",
        "    print(\"Headline:\", article['headline'])\n",
        "    print(\"Publication Date:\", article['publication_date'])\n",
        "    print(\"Content:\", article['content'])\n",
        "    print(\"--------------\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99440176-8438-4d0c-eee9-5e8d2e7c4cb9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total articles found: 1\n",
            "---------------\n",
            "Headline: Nvidia's ripple effect\n",
            "Publication Date: None\n",
            "Content: The leading AI chipmaker has seen its shares rise by almost 50% this year. Now, the company is spreading that love to other AI firms.\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bc8d48-036d-483b-8a14-f48e37a7e72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            "Title: Artificial intelligence in medicine\n",
            "Venue: P Hamet, J Tremblay\n",
            "Year: 2017 - Elsevier\n",
            "Authors: Metabolism, 2017\n",
            "Abstract: … artificial intelligence” (AI) in 1955, defining it as “the science and engineering of making \n",
            "intelligent … at a Dartmouth College conference on artificial intelligence. The conference gave birth …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 2:\n",
            "Title: Causability and explainability of artificial intelligence in medicine\n",
            "Venue: A Holzinger, G Langs, H Denk…\n",
            "Year: 2019 - Wiley Online Library\n",
            "Authors: … Reviews: Data Mining …, 2019\n",
            "Abstract: Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the \n",
            "problem of explainability is as old as AI itself and classic AI represented comprehensible …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 3:\n",
            "Title: [BOOK][B] Introduction to artificial intelligence\n",
            "Venue: W Ertel\n",
            "Year: W Ertel - 2018 - books.google.com\n",
            "Authors: 2018\n",
            "Abstract: … during your studies this book will help you share my fascination with Artificial Intelligence. … \n",
            "philosophical questions surrounding intelligence and artificial intelligence. We humans have …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 4:\n",
            "Title: [PDF][PDF] Artificial intelligence, for real\n",
            "Venue: E Brynjolfsson, A Mcafee\n",
            "Year: 2017 - starlab-alliance.com\n",
            "Authors: Harvard business review, 2017\n",
            "Abstract: … technology of our era is artificial intelligence, particularly machine learning (ML) — that is, the \n",
            "… Within just the past few years machine learning has become far more effective and widely …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 5:\n",
            "Title: [BOOK][B] The handbook of artificial intelligence: Volume 3\n",
            "Venue: PR Cohen, EA Feigenbaum\n",
            "Year: EA Feigenbaum - 2014 - books.google.com\n",
            "Authors: 2014\n",
            "Abstract: … The third approach, the one we call Artificial Intelligence, was an attempt to build intelligent \n",
            "machines without any prejudice toward making the system simple, biological, or humanoid. (p…\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 6:\n",
            "Title: Brain intelligence: go beyond artificial intelligence\n",
            "Venue: H Lu, Y Li, M Chen, H Kim, S Serikawa\n",
            "Year: 2018 - Springer\n",
            "Authors: Mobile Networks and Applications, 2018\n",
            "Abstract: … artificial intelligence technology, we aim to develop a new concept of general-purpose \n",
            "intelligence … , we plan to develop an intelligent learning model called “Brain Intelligence (BI)” that …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 7:\n",
            "Title: [HTML][HTML] Explanation in artificial intelligence: Insights from the social sciences\n",
            "Venue: T Miller\n",
            "Year: 2019 - Elsevier\n",
            "Authors: Artificial intelligence, 2019\n",
            "Abstract: … in artificial intelligence. However, it is fair to say that most work in explainable artificial \n",
            "intelligence … This paper argues that the field of explainable artificial intelligence can build on this …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 8:\n",
            "Title: [PDF][PDF] Artificial intelligence for the real world\n",
            "Venue: TH Davenport, R Ronanki\n",
            "Year: 2018 - blockqai.com\n",
            "Authors: Harvard business review, 2018\n",
            "Abstract: … surrounding artificial intelligence … , intelligent agents, and machine learning were the least \n",
            "common type in our study (accounting for 16% of the total). This category includes: • intelligent …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 9:\n",
            "Title: [HTML][HTML] Overview of artificial intelligence in medicine\n",
            "Venue: P Malik, M Pathania, VK Rathaur\n",
            "Year: 2019 - ncbi.nlm.nih.gov\n",
            "Authors: Journal of family medicine …, 2019\n",
            "Abstract: … on the fact that the intelligent behavior of a computer is … Artificial intelligent techniques such \n",
            "as fuzzy expert systems, Bayesian networks, artificial neural networks, and hybrid intelligent …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 10:\n",
            "Title: [BOOK][B] Readings in distributed artificial intelligence\n",
            "Venue: AH Bond, L Gasser\n",
            "Year: L Gasser - 2014 - books.google.com\n",
            "Authors: 2014\n",
            "Abstract: … and of this book is distributed artificially intelligent systems. We shall examine what … artificial \n",
            "intelligence. The conceptual basis for concurrent processes underlying artificial intelligence …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 11:\n",
            "Title: DARPA's explainable artificial intelligence (XAI) program\n",
            "Venue: D Gunning, D Aha\n",
            "Year: 2019 - ojs.aaai.org\n",
            "Authors: AI magazine, 2019\n",
            "Abstract: … , whose challenges require developing more intelligent, autonomous, and symbiotic … artificially \n",
            "intelligent partners. To address this, DARPA launched its explainable artificial intelligence …\n",
            "\n",
            "==================================================\n",
            "\n",
            "Article 12:\n",
            "Title: Artificial intelligence in radiology\n",
            "Venue: A Hosny, C Parmar, J Quackenbush…\n",
            "Year: 2018 - nature.com\n",
            "Authors: Nature Reviews …, 2018\n",
            "Abstract: Artificial intelligence (AI) algorithms, particularly deep learning, have demonstrated \n",
            "remarkable progress in image-recognition tasks. Methods ranging from convolutional neural …\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#I cannot able to load the data of 1000 articles because of CPU and time usage to run the code , Code may take lot of time to process data\n",
        "#so I have retrieved just 12 articles, If i change num_article parameter below to 1000 ,i would get 1000 articles data.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_google_scholar_articles(query, num_articles):\n",
        "    articles = []\n",
        "    start = 0\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        url = f\"https://scholar.google.com/scholar?q={query}&start={start}&as_ylo=2014&as_yhi=2024&hl=en&as_sdt=0%2C5\"\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for result in soup.find_all('div', class_='gs_ri'):\n",
        "            title = result.find('h3', class_='gs_rt').text.strip()\n",
        "            venue = result.find('div', class_='gs_a').text.split('-')[0].strip()\n",
        "            year = result.find('div', class_='gs_a').text.split(',')[-1].strip()\n",
        "            authors = result.find('div', class_='gs_a').text.split('-')[1].strip()\n",
        "            abstract = result.find('div', class_='gs_rs').text.strip()\n",
        "\n",
        "            articles.append({\n",
        "                'title': title,\n",
        "                'venue': venue,\n",
        "                'year': year,\n",
        "                'authors': authors,\n",
        "                'abstract': abstract\n",
        "            })\n",
        "\n",
        "            if len(articles) >= num_articles:\n",
        "                break\n",
        "\n",
        "        start += 10  # Move to the next page\n",
        "\n",
        "    return articles[:num_articles]\n",
        "\n",
        "query = \"artificial intelligence\"\n",
        "num_articles = 12  # change the number of articles count whichever you want\n",
        "articles = fetch_google_scholar_articles(query, num_articles)\n",
        "\n",
        "for idx, article in enumerate(articles, 1):\n",
        "    print(f\"Article {idx}:\")\n",
        "    print(f\"Title: {article['title']}\")\n",
        "    print(f\"Venue: {article['venue']}\")\n",
        "    print(f\"Year: {article['year']}\")\n",
        "    print(f\"Authors: {article['authors']}\")\n",
        "    print(f\"Abstract: {article['abstract']}\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\") # Add a separator between articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7abfa78a-9128-4f62-90d4-63ddd4cddf59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a keyword to search for on Wikipedia: india\n",
            "Extracted 10 Wikipedia articles for keyword: india\n",
            "                                   Title  \\\n",
            "0                                  India   \n",
            "1  States and union territories of India   \n",
            "2                     East India Company   \n",
            "3                    Government of India   \n",
            "4                  Demographics of India   \n",
            "5                          Punjab, India   \n",
            "6                       Economy of India   \n",
            "7                Prime Minister of India   \n",
            "8            India national cricket team   \n",
            "9                       History of India   \n",
            "\n",
            "                                             Extract   Page_ID  \\\n",
            "0  <span class=\"searchmatch\">India</span>, offici...     14533   \n",
            "1  <span class=\"searchmatch\">India</span> is a fe...    375986   \n",
            "2  The East <span class=\"searchmatch\">India</span...     43281   \n",
            "3  The Government of <span class=\"searchmatch\">In...    553883   \n",
            "4  <span class=\"searchmatch\">India</span> is the ...     14598   \n",
            "5  historically known as Panchanada or Pentapotam...  23397776   \n",
            "6  The economy of <span class=\"searchmatch\">India...   1472206   \n",
            "7  The prime minister of <span class=\"searchmatch...     24452   \n",
            "8  The <span class=\"searchmatch\">India</span> men...    407754   \n",
            "9  between 2500 BCE and 1900 BCE in present-day P...     13890   \n",
            "\n",
            "                                                 URL  \n",
            "0                https://en.wikipedia.org/wiki/India  \n",
            "1  https://en.wikipedia.org/wiki/States_and_union...  \n",
            "2   https://en.wikipedia.org/wiki/East_India_Company  \n",
            "3  https://en.wikipedia.org/wiki/Government_of_India  \n",
            "4  https://en.wikipedia.org/wiki/Demographics_of_...  \n",
            "5        https://en.wikipedia.org/wiki/Punjab,_India  \n",
            "6     https://en.wikipedia.org/wiki/Economy_of_India  \n",
            "7  https://en.wikipedia.org/wiki/Prime_Minister_o...  \n",
            "8  https://en.wikipedia.org/wiki/India_national_c...  \n",
            "9     https://en.wikipedia.org/wiki/History_of_India  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
            "<ipython-input-19-e3bbe9f43159>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(row, index=cols), ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "#Due to authentication API keys issues with reddit api , twitter API, I have used publicly accessed wikepedia to write code\n",
        "#I dont have enough time to get API keys from the those organizations, so i have retrived data from wikepedia\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "cols = ['Title', 'Extract', 'Page_ID', 'URL']\n",
        "data = pd.DataFrame(columns=cols)\n",
        "keyword = input(\"Enter a keyword to search for on Wikipedia: \")\n",
        "base_url = f\"https://en.wikipedia.org/w/api.php\"\n",
        "params = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"search\",\n",
        "    \"srsearch\": keyword,\n",
        "    \"utf8\": 1\n",
        "}\n",
        "\n",
        "try:\n",
        "\n",
        "    res = requests.get(base_url, params=params)\n",
        "    res.raise_for_status()\n",
        "    json_data = res.json()\n",
        "    for item in json_data['query']['search']:\n",
        "        title = item['title']\n",
        "        page_id = item['pageid']\n",
        "        url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
        "        extract = item.get('snippet', 'No snippet available')\n",
        "        row = [title, extract, page_id, url]\n",
        "        data = data.append(pd.Series(row, index=cols), ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f\"Extracted {len(data)} Wikipedia articles for keyword: {keyword}\")\n",
        "    print(data)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"Error occurred while making the request:\", e)\n",
        "except KeyError:\n",
        "    print(\"Error: Unexpected response format from Wikipedia API.\")\n",
        "except ValueError:\n",
        "    print(\"Error: Failed to parse JSON response from Wikipedia API.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Questions were too tough for my knowledge and so I took some help from chatgpt.\n",
        "#I felt very tough with the python to do web scraping,\n",
        "#because I was doing web scraping with rapid miner in other class INFO 5810 in this semester,when comapred it is so easy to web scrap with rapid minor\n",
        "#Since i am not well known with python programming , I am still strugling to write complete code witout errors.\n",
        "# I have faced some issues when accessing somewebsites like reddit API, Twitter API because I need to get API keys from respective organizations to authorize.\n"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}